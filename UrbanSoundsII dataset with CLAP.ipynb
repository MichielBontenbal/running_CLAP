{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97a13bc",
   "metadata": {},
   "source": [
    "# Audio classification on the Urban SoundsII dataset with CLAP\n",
    "\n",
    "###  Goal of the notebook\n",
    "In this notebook you can do audio classification with CLAP.\n",
    "\n",
    "! Warning: This notebook works with Python 3.12 !!!\n",
    "(The datasets library have many breaking changes so be careful)\n",
    "\n",
    "### CLAP\n",
    "CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task.\n",
    "\n",
    "Source: https://huggingface.co/laion/larger_clap_general\n",
    "CLAP paper: https://arxiv.org/abs/2211.06687\n",
    "\n",
    "In this notebook we will use two CLAP models:\n",
    "1. larger_clap_music_and_speech\n",
    "2. larger_clap_general\n",
    "\n",
    "### Using ðŸ¤— datasets and ðŸ¤—transformers\n",
    "The dataset is hosted on the Huggingface Hub at: https://huggingface.co/datasets/MichielBontenbal/UrbanSoundsII\n",
    "\n",
    "(This is a new version of the same dataset as the old dataset got corrupted.)\n",
    "\n",
    "This dataset contains nine classes of audio events in an urban environment. \n",
    "\n",
    "In this notebook we will use ðŸ¤—  ```dataset``` library to load this dataset. \n",
    "\n",
    "And we'll use the ðŸ¤— ```transformers``` library to run the CLAP model. Please find more info: https://huggingface.co/docs/transformers/model_doc/clap \n",
    "\n",
    "\n",
    "### Contents\n",
    "0. Install packages & check versions\n",
    "1. Inspect the dataset\n",
    "2. Run CLAP on the Urban Sounds Amsterdam dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d66e31",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24640679",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc99f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\\[audio\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e18c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87dca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check python version. Make sure you have 3.12 installed!\n",
    "import platform\n",
    "platform.python_version()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95407312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f'numpy version: {np.__version__}')\n",
    "import soundfile\n",
    "print(f'soundfile version: {soundfile.__version__}')\n",
    "import librosa\n",
    "print(f'librosa version: {librosa.__version__}')\n",
    "import IPython\n",
    "print(f'IPython version: {IPython.__version__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "print(f'datasets version: {datasets.__version__}')\n",
    "import transformers\n",
    "print(f'transformers version: {transformers.__version__}')\n",
    "import torch\n",
    "print(f'torch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4888e0",
   "metadata": {},
   "source": [
    "## 1. Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b91b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"MichielBontenbal/UrbanSoundsII\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ESC50 dataset is one of the very few other datasets on Environmental Sound classification\n",
    "#You could try this as an alternative\n",
    "#dataset = load_dataset(\"confit/esc50-demo\", \"fold1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the dataset\n",
    "#dataset = ds\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect one sample from the dataset\n",
    "example = dataset['train']['audio'][0]\n",
    "label = dataset['train']['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612c868",
   "metadata": {},
   "source": [
    "You may notice that the audio column contains several features. Hereâ€™s what they are:\n",
    "\n",
    "- path: the path to the downloaded (and converted) audio file\n",
    "- array: The decoded audio data, represented as a 1-dimensional NumPy array.\n",
    "- sampling_rate. The sampling rate of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the label data\n",
    "print(dataset['train']['label'])\n",
    "print(len(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb34b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspecting the audio array\n",
    "array = dataset[\"train\"][\"audio\"][0][\"array\"]\n",
    "sampling_rate = example[\"sampling_rate\"]\n",
    "print(array.shape)\n",
    "print(array)\n",
    "print(type(array))\n",
    "print(sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1deef",
   "metadata": {},
   "source": [
    "## 2. CLAP on the Urban Sounds Amsterdam dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a random number to select from dataset\n",
    "import random\n",
    "\n",
    "random_number = random.randint(0, len(dataset['train']['audio']))\n",
    "random_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0c2b73",
   "metadata": {},
   "source": [
    "### Runnning it with \"Larger CLAP music and speech\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to load a random number out of the dataset\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import IPython\n",
    "\n",
    "example=dataset['train']['audio'][random_number]\n",
    "audio = dataset[\"train\"][\"audio\"][random_number][\"array\"]\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_music_and_speech\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Motorcycle\", \"Moped\", 'Claxon','Alarm', 'Silence','Loud people','Talking','Gunshot', 'Slamming door','Music'])\n",
    "print(output[0],'\\n',output[1])\n",
    "print(random_number)\n",
    "IPython.display.Audio(example[\"array\"], rate=example['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304d2e8",
   "metadata": {},
   "source": [
    "### Runnning it with \"Larger CLAP general\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4171af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is code to convert the given labels (0,1,2,3,4,5,6,7,8,9) to a real string. \n",
    "# create a dictionary the converts the class folders to real names\n",
    "label_dict ={0:'Gunshot', 1:'Moped alarm', 2:'Moped', 3:'Claxon', 4:'Slamming door', 5:'Screaming', 6:'Motorcycle', 7:'Talking', 8:'Music'}\n",
    "print('The given labels are: ')\n",
    "for i in range(0,9):\n",
    "    print(label_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4328d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger_clap_general\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import IPython\n",
    "\n",
    "example=dataset['train']['audio'][random_number]\n",
    "audio = dataset[\"train\"][\"audio\"][random_number]['array']\n",
    "\n",
    "audio_classifier = pipeline(task=\"zero-shot-audio-classification\", model=\"laion/larger_clap_general\")\n",
    "output = audio_classifier(audio, candidate_labels=[\"Gunshot\", \"Moped\", 'Moped alarm','Claxon','Screaming', 'Motorcycle','Talking', 'Slamming door','Music', 'Silence'])\n",
    "\n",
    "predicted_label = output[0]['label']\n",
    "print(f'Prediction: {predicted_label}')\n",
    "\n",
    "label_name =label_dict[dataset['train']['label'][i]]\n",
    "print(f'The given label: {label_name}')\n",
    "\n",
    "if label_name == output[0]['label']:\n",
    "    print(\"This is correct\")\n",
    "else:\n",
    "    print('This is false')\n",
    "print(f'Probability: {round(output[0][\"score\"],3)}')\n",
    "\n",
    "IPython.display.Audio(example['array'], rate=example['sampling_rate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
